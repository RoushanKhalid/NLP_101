{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq2KBUfzPgKt"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdHfojykE_r1",
        "outputId": "af5f08fd-ad41-42dc-ff4b-ccfae9fdf13e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWCjfUxpFwDE"
      },
      "outputs": [],
      "source": [
        "corpus='''John Smith lives in New York City. He works at Google as a software engineer. The weather in Paris is beautiful during the summer.\n",
        "Elon Musk founded SpaceX and is the CEO of Tesla. Wow! That's awesome.Python is a popular programming language for data science and machine learning. Amazon was founded by Jeff Bezos in 1994 in Seattle. The capital of Japan is Tokyo, which is known for its technology and culture.\n",
        "Messi scored two goals in the FIFA World Cup final. Apple Inc. released the new iPhone 15 in September 2023. Students from Harvard University won the international robotics competition. ChatGPT is an advanced AI model developed by OpenAI.'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "WCzaOqhiHhCK",
        "outputId": "4ebca29d-d073-4415-83f4-dfa5a05227e2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"John Smith lives in New York City. He works at Google as a software engineer. The weather in Paris is beautiful during the summer. \\nElon Musk founded SpaceX and is the CEO of Tesla. Wow! That's awesome.Python is a popular programming language for data science and machine learning. Amazon was founded by Jeff Bezos in 1994 in Seattle. The capital of Japan is Tokyo, which is known for its technology and culture. \\nMessi scored two goals in the FIFA World Cup final. Apple Inc. released the new iPhone 15 in September 2023. Students from Harvard University won the international robotics competition. ChatGPT is an advanced AI model developed by OpenAI.\""
            ]
          },
          "execution_count": 303,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlZVMkmbHwNe",
        "outputId": "b9e1487b-0062-4ba8-e3bb-9b137289c08e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "John Smith lives in New York City. He works at Google as a software engineer. The weather in Paris is beautiful during the summer. \n",
            "Elon Musk founded SpaceX and is the CEO of Tesla. Wow! That's awesome.Python is a popular programming language for data science and machine learning. Amazon was founded by Jeff Bezos in 1994 in Seattle. The capital of Japan is Tokyo, which is known for its technology and culture. \n",
            "Messi scored two goals in the FIFA World Cup final. Apple Inc. released the new iPhone 15 in September 2023. Students from Harvard University won the international robotics competition. ChatGPT is an advanced AI model developed by OpenAI.\n"
          ]
        }
      ],
      "source": [
        "print(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_0FoqnCH5Y1"
      },
      "source": [
        "**Paragraph into Sentence**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRozo7sLICT4"
      },
      "outputs": [],
      "source": [
        "from nltk import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2fc3bb5",
        "outputId": "1a6b185a-74ee-494e-99ef-54919a5d02d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 306,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPKA6FT2ILES",
        "outputId": "1c870d86-e9a0-4937-d9d6-9eb3a9e7eaff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['John Smith lives in New York City.',\n",
              " 'He works at Google as a software engineer.',\n",
              " 'The weather in Paris is beautiful during the summer.',\n",
              " 'Elon Musk founded SpaceX and is the CEO of Tesla.',\n",
              " 'Wow!',\n",
              " \"That's awesome.Python is a popular programming language for data science and machine learning.\",\n",
              " 'Amazon was founded by Jeff Bezos in 1994 in Seattle.',\n",
              " 'The capital of Japan is Tokyo, which is known for its technology and culture.',\n",
              " 'Messi scored two goals in the FIFA World Cup final.',\n",
              " 'Apple Inc. released the new iPhone 15 in September 2023.',\n",
              " 'Students from Harvard University won the international robotics competition.',\n",
              " 'ChatGPT is an advanced AI model developed by OpenAI.']"
            ]
          },
          "execution_count": 307,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sent_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLszHCATLQm6"
      },
      "source": [
        "**Listing the Sentences**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70ad6de5",
        "outputId": "8589cec3-2cca-4005-b9e9-461cd340e15d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['John Smith lives in New York City.', 'He works at Google as a software engineer.', 'The weather in Paris is beautiful during the summer.', 'Elon Musk founded SpaceX and is the CEO of Tesla.', 'Wow!', \"That's awesome.Python is a popular programming language for data science and machine learning.\", 'Amazon was founded by Jeff Bezos in 1994 in Seattle.', 'The capital of Japan is Tokyo, which is known for its technology and culture.', 'Messi scored two goals in the FIFA World Cup final.', 'Apple Inc. released the new iPhone 15 in September 2023.', 'Students from Harvard University won the international robotics competition.', 'ChatGPT is an advanced AI model developed by OpenAI.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "sentences = nltk.sent_tokenize(corpus)\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3310695",
        "outputId": "0a8d7141-4d7f-4eb8-f727-285f5a777225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['John Smith lives in New York City.', 'He works at Google as a software engineer.', 'The weather in Paris is beautiful during the summer.', 'Elon Musk founded SpaceX and is the CEO of Tesla.', 'Wow!', \"That's awesome.Python is a popular programming language for data science and machine learning.\", 'Amazon was founded by Jeff Bezos in 1994 in Seattle.', 'The capital of Japan is Tokyo, which is known for its technology and culture.', 'Messi scored two goals in the FIFA World Cup final.', 'Apple Inc.', 'released the new iPhone 15 in September 2023.', 'Students from Harvard University won the international robotics competition.', 'ChatGPT is an advanced AI model developed by OpenAI.']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "# Download the punkt tokenizer if not already downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the punkt tokenizer\n",
        "tokenizer = PunktSentenceTokenizer()\n",
        "\n",
        "# Tokenize the corpus into sentences\n",
        "sentences = tokenizer.tokenize(corpus)\n",
        "\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2wJkiWtKOGq",
        "outputId": "c013605b-3e0d-4438-c5ca-0516da8923a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 310,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cN-PwZXvKSOj",
        "outputId": "6aefe6f0-bff3-4de7-c023-37997cce66d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "John Smith lives in New York City.\n",
            "He works at Google as a software engineer.\n",
            "The weather in Paris is beautiful during the summer.\n",
            "Elon Musk founded SpaceX and is the CEO of Tesla.\n",
            "Wow!\n",
            "That's awesome.Python is a popular programming language for data science and machine learning.\n",
            "Amazon was founded by Jeff Bezos in 1994 in Seattle.\n",
            "The capital of Japan is Tokyo, which is known for its technology and culture.\n",
            "Messi scored two goals in the FIFA World Cup final.\n",
            "Apple Inc.\n",
            "released the new iPhone 15 in September 2023.\n",
            "Students from Harvard University won the international robotics competition.\n",
            "ChatGPT is an advanced AI model developed by OpenAI.\n"
          ]
        }
      ],
      "source": [
        "for sentence in sentences:\n",
        "  print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfuWM4N4LkXk"
      },
      "outputs": [],
      "source": [
        "from nltk import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfPVoTrFLou9",
        "outputId": "82266fe1-d6f0-4fb5-f39d-f5a65e4e6ead"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['John',\n",
              " 'Smith',\n",
              " 'lives',\n",
              " 'in',\n",
              " 'New',\n",
              " 'York',\n",
              " 'City',\n",
              " '.',\n",
              " 'He',\n",
              " 'works',\n",
              " 'at',\n",
              " 'Google',\n",
              " 'as',\n",
              " 'a',\n",
              " 'software',\n",
              " 'engineer',\n",
              " '.',\n",
              " 'The',\n",
              " 'weather',\n",
              " 'in',\n",
              " 'Paris',\n",
              " 'is',\n",
              " 'beautiful',\n",
              " 'during',\n",
              " 'the',\n",
              " 'summer',\n",
              " '.',\n",
              " 'Elon',\n",
              " 'Musk',\n",
              " 'founded',\n",
              " 'SpaceX',\n",
              " 'and',\n",
              " 'is',\n",
              " 'the',\n",
              " 'CEO',\n",
              " 'of',\n",
              " 'Tesla',\n",
              " '.',\n",
              " 'Wow',\n",
              " '!',\n",
              " 'That',\n",
              " \"'s\",\n",
              " 'awesome.Python',\n",
              " 'is',\n",
              " 'a',\n",
              " 'popular',\n",
              " 'programming',\n",
              " 'language',\n",
              " 'for',\n",
              " 'data',\n",
              " 'science',\n",
              " 'and',\n",
              " 'machine',\n",
              " 'learning',\n",
              " '.',\n",
              " 'Amazon',\n",
              " 'was',\n",
              " 'founded',\n",
              " 'by',\n",
              " 'Jeff',\n",
              " 'Bezos',\n",
              " 'in',\n",
              " '1994',\n",
              " 'in',\n",
              " 'Seattle',\n",
              " '.',\n",
              " 'The',\n",
              " 'capital',\n",
              " 'of',\n",
              " 'Japan',\n",
              " 'is',\n",
              " 'Tokyo',\n",
              " ',',\n",
              " 'which',\n",
              " 'is',\n",
              " 'known',\n",
              " 'for',\n",
              " 'its',\n",
              " 'technology',\n",
              " 'and',\n",
              " 'culture',\n",
              " '.',\n",
              " 'Messi',\n",
              " 'scored',\n",
              " 'two',\n",
              " 'goals',\n",
              " 'in',\n",
              " 'the',\n",
              " 'FIFA',\n",
              " 'World',\n",
              " 'Cup',\n",
              " 'final',\n",
              " '.',\n",
              " 'Apple',\n",
              " 'Inc.',\n",
              " 'released',\n",
              " 'the',\n",
              " 'new',\n",
              " 'iPhone',\n",
              " '15',\n",
              " 'in',\n",
              " 'September',\n",
              " '2023',\n",
              " '.',\n",
              " 'Students',\n",
              " 'from',\n",
              " 'Harvard',\n",
              " 'University',\n",
              " 'won',\n",
              " 'the',\n",
              " 'international',\n",
              " 'robotics',\n",
              " 'competition',\n",
              " '.',\n",
              " 'ChatGPT',\n",
              " 'is',\n",
              " 'an',\n",
              " 'advanced',\n",
              " 'AI',\n",
              " 'model',\n",
              " 'developed',\n",
              " 'by',\n",
              " 'OpenAI',\n",
              " '.']"
            ]
          },
          "execution_count": 313,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PC4zJLxHL-9Q",
        "outputId": "3c697892-b54c-41a8-ee7e-fa27d59f4051"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['John', 'Smith', 'lives', 'in', 'New', 'York', 'City', '.']\n",
            "['He', 'works', 'at', 'Google', 'as', 'a', 'software', 'engineer', '.']\n",
            "['The', 'weather', 'in', 'Paris', 'is', 'beautiful', 'during', 'the', 'summer', '.']\n",
            "['Elon', 'Musk', 'founded', 'SpaceX', 'and', 'is', 'the', 'CEO', 'of', 'Tesla', '.']\n",
            "['Wow', '!']\n",
            "['That', \"'s\", 'awesome.Python', 'is', 'a', 'popular', 'programming', 'language', 'for', 'data', 'science', 'and', 'machine', 'learning', '.']\n",
            "['Amazon', 'was', 'founded', 'by', 'Jeff', 'Bezos', 'in', '1994', 'in', 'Seattle', '.']\n",
            "['The', 'capital', 'of', 'Japan', 'is', 'Tokyo', ',', 'which', 'is', 'known', 'for', 'its', 'technology', 'and', 'culture', '.']\n",
            "['Messi', 'scored', 'two', 'goals', 'in', 'the', 'FIFA', 'World', 'Cup', 'final', '.']\n",
            "['Apple', 'Inc', '.']\n",
            "['released', 'the', 'new', 'iPhone', '15', 'in', 'September', '2023', '.']\n",
            "['Students', 'from', 'Harvard', 'University', 'won', 'the', 'international', 'robotics', 'competition', '.']\n",
            "['ChatGPT', 'is', 'an', 'advanced', 'AI', 'model', 'developed', 'by', 'OpenAI', '.']\n"
          ]
        }
      ],
      "source": [
        "for sentence in sentences:\n",
        "  print(word_tokenize(sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUc_-dLgMHw4"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import wordpunct_tokenize # Making perfect punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zE_P0EouMTp6",
        "outputId": "ccbb0b86-848a-4a5c-92bd-d1455b352454"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['John', 'Smith', 'lives', 'in', 'New', 'York', 'City', '.']\n",
            "['He', 'works', 'at', 'Google', 'as', 'a', 'software', 'engineer', '.']\n",
            "['The', 'weather', 'in', 'Paris', 'is', 'beautiful', 'during', 'the', 'summer', '.']\n",
            "['Elon', 'Musk', 'founded', 'SpaceX', 'and', 'is', 'the', 'CEO', 'of', 'Tesla', '.']\n",
            "['Wow', '!']\n",
            "['That', \"'\", 's', 'awesome', '.', 'Python', 'is', 'a', 'popular', 'programming', 'language', 'for', 'data', 'science', 'and', 'machine', 'learning', '.']\n",
            "['Amazon', 'was', 'founded', 'by', 'Jeff', 'Bezos', 'in', '1994', 'in', 'Seattle', '.']\n",
            "['The', 'capital', 'of', 'Japan', 'is', 'Tokyo', ',', 'which', 'is', 'known', 'for', 'its', 'technology', 'and', 'culture', '.']\n",
            "['Messi', 'scored', 'two', 'goals', 'in', 'the', 'FIFA', 'World', 'Cup', 'final', '.']\n",
            "['Apple', 'Inc', '.']\n",
            "['released', 'the', 'new', 'iPhone', '15', 'in', 'September', '2023', '.']\n",
            "['Students', 'from', 'Harvard', 'University', 'won', 'the', 'international', 'robotics', 'competition', '.']\n",
            "['ChatGPT', 'is', 'an', 'advanced', 'AI', 'model', 'developed', 'by', 'OpenAI', '.']\n"
          ]
        }
      ],
      "source": [
        "for sentence in sentences:\n",
        "  print(wordpunct_tokenize(sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1gIylJcMkgt"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer # Check the last word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRcWVHp1Mtla"
      },
      "outputs": [],
      "source": [
        "tokenizer = TreebankWordTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu0R1TT6M8c4",
        "outputId": "728ab06c-a244-45fc-f0b4-3a1d7f20a151"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['John',\n",
              " 'Smith',\n",
              " 'lives',\n",
              " 'in',\n",
              " 'New',\n",
              " 'York',\n",
              " 'City.',\n",
              " 'He',\n",
              " 'works',\n",
              " 'at',\n",
              " 'Google',\n",
              " 'as',\n",
              " 'a',\n",
              " 'software',\n",
              " 'engineer.',\n",
              " 'The',\n",
              " 'weather',\n",
              " 'in',\n",
              " 'Paris',\n",
              " 'is',\n",
              " 'beautiful',\n",
              " 'during',\n",
              " 'the',\n",
              " 'summer.',\n",
              " 'Elon',\n",
              " 'Musk',\n",
              " 'founded',\n",
              " 'SpaceX',\n",
              " 'and',\n",
              " 'is',\n",
              " 'the',\n",
              " 'CEO',\n",
              " 'of',\n",
              " 'Tesla.',\n",
              " 'Wow',\n",
              " '!',\n",
              " 'That',\n",
              " \"'s\",\n",
              " 'awesome.Python',\n",
              " 'is',\n",
              " 'a',\n",
              " 'popular',\n",
              " 'programming',\n",
              " 'language',\n",
              " 'for',\n",
              " 'data',\n",
              " 'science',\n",
              " 'and',\n",
              " 'machine',\n",
              " 'learning.',\n",
              " 'Amazon',\n",
              " 'was',\n",
              " 'founded',\n",
              " 'by',\n",
              " 'Jeff',\n",
              " 'Bezos',\n",
              " 'in',\n",
              " '1994',\n",
              " 'in',\n",
              " 'Seattle.',\n",
              " 'The',\n",
              " 'capital',\n",
              " 'of',\n",
              " 'Japan',\n",
              " 'is',\n",
              " 'Tokyo',\n",
              " ',',\n",
              " 'which',\n",
              " 'is',\n",
              " 'known',\n",
              " 'for',\n",
              " 'its',\n",
              " 'technology',\n",
              " 'and',\n",
              " 'culture.',\n",
              " 'Messi',\n",
              " 'scored',\n",
              " 'two',\n",
              " 'goals',\n",
              " 'in',\n",
              " 'the',\n",
              " 'FIFA',\n",
              " 'World',\n",
              " 'Cup',\n",
              " 'final.',\n",
              " 'Apple',\n",
              " 'Inc.',\n",
              " 'released',\n",
              " 'the',\n",
              " 'new',\n",
              " 'iPhone',\n",
              " '15',\n",
              " 'in',\n",
              " 'September',\n",
              " '2023.',\n",
              " 'Students',\n",
              " 'from',\n",
              " 'Harvard',\n",
              " 'University',\n",
              " 'won',\n",
              " 'the',\n",
              " 'international',\n",
              " 'robotics',\n",
              " 'competition.',\n",
              " 'ChatGPT',\n",
              " 'is',\n",
              " 'an',\n",
              " 'advanced',\n",
              " 'AI',\n",
              " 'model',\n",
              " 'developed',\n",
              " 'by',\n",
              " 'OpenAI',\n",
              " '.']"
            ]
          },
          "execution_count": 319,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.tokenize(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SLQ5sWsOc90"
      },
      "source": [
        "**Difference**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpRQupu8NbfT"
      },
      "source": [
        "In NLP using NLTK, `sent_tokenize` is a simple function that uses the `PunktSentenceTokenizer` under the hood to split a paragraph into sentences based on pre-trained models and punctuation. `PunktSentenceTokenizer` is the actual class that enables unsupervised sentence boundary detection, allowing for more customization.\n",
        "\n",
        "On the other hand, `word_tokenize` is a high-level function that splits text into words and punctuation using the Penn Treebank rules. `wordpunct_tokenize` is simpler—it splits words on all punctuation, so contractions like “don’t” become `['don', ''', 't']`.\n",
        "\n",
        "Meanwhile, `TreebankWordTokenizer` is more precise and uses the Treebank rules to tokenize text like `word_tokenize` but offers more control, such as splitting “can’t” into `['ca', \"n't\"]` and handling punctuation consistently.\n",
        "\n",
        "Each tokenizer serves different use cases based on the level of accuracy or simplicity required.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xGjzxYxgYbf"
      },
      "source": [
        "# Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "744jDuEvgt_J"
      },
      "outputs": [],
      "source": [
        "words = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16esrL1dhv6S"
      },
      "source": [
        "**Porter Stemmer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_whFLH-hiT4"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qT8UZRpsh3uT"
      },
      "outputs": [],
      "source": [
        "porter_stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFy9S6lLiJj-",
        "outputId": "a03ea732-577f-449b-b59f-5a4dea6ccb46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "program --> program\n",
            "programs --> program\n",
            "programer --> program\n",
            "programing --> program\n",
            "programers --> program\n"
          ]
        }
      ],
      "source": [
        "for word in words:\n",
        "  print(word+\" --> \"+porter_stemmer.stem(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xw_5hLn-iwvX",
        "outputId": "ed5ed892-e435-4bf4-a02e-30900c3a47bd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'histori'"
            ]
          },
          "execution_count": 324,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "porter_stemmer.stem(\"history\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1l2oBTXEjBXF",
        "outputId": "e90d05f0-ded2-4be4-dcb4-dba0ecdfcfb9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'congratul'"
            ]
          },
          "execution_count": 325,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "porter_stemmer.stem(\"congratulations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdFU-Pk7jTzn"
      },
      "source": [
        "**RegexpStemmer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b76b9a8"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import RegexpStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arOCns_CkGtX"
      },
      "outputs": [],
      "source": [
        "regexp_stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "n-nml7zxkSK4",
        "outputId": "a1a03438-db24-4cf0-81f6-d901c31222f3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'eat'"
            ]
          },
          "execution_count": 328,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "regexp_stemmer.stem(\"eating\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KmiWD7fYkgjk",
        "outputId": "3d3a72b9-317a-4ce0-fc08-addf23d5c586"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'explain'"
            ]
          },
          "execution_count": 329,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "regexp_stemmer.stem(\"explainable\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rGG5sM14kwJT",
        "outputId": "b565d9dd-e8d6-4b69-cf37-880f96ce29bb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'irritat'"
            ]
          },
          "execution_count": 330,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "regexp_stemmer.stem(\"irritating\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP5PX0q1lAsN"
      },
      "source": [
        "**Snowball Stemmer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbJVel-KlY0A"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import SnowballStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Jt1Xu65lccp"
      },
      "outputs": [],
      "source": [
        "snowball_stemmer = SnowballStemmer('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhYeud6WlivO",
        "outputId": "1ff303f6-ede7-472c-e35b-305c6e49aa1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "program --> program\n",
            "programs --> program\n",
            "programer --> program\n",
            "programing --> program\n",
            "programers --> program\n"
          ]
        }
      ],
      "source": [
        "for word in words:\n",
        "  print(word+\" --> \"+snowball_stemmer.stem(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SBd-NOxGlu5L",
        "outputId": "80d7f02b-8793-458f-f9e8-6081872155fa"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'histori'"
            ]
          },
          "execution_count": 334,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "snowball_stemmer.stem(\"history\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "F_7fjcbal0Yc",
        "outputId": "1311f408-ab60-44b4-8610-c3134050e6fd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'irrit'"
            ]
          },
          "execution_count": 335,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "snowball_stemmer.stem(\"irritating\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLjRUMthmHoe",
        "outputId": "76fe1618-937d-4d42-a340-22ce4dcd135f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('fairli', 'sportingli', 'goe')"
            ]
          },
          "execution_count": 336,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "porter_stemmer.stem(\"fairly\"), porter_stemmer.stem(\"sportingly\"), porter_stemmer.stem(\"goes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr_h5LVimeZa",
        "outputId": "aca1f1c9-f699-4c74-b66e-53bcae37f55e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('fair', 'sport', 'goe')"
            ]
          },
          "execution_count": 337,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "snowball_stemmer.stem(\"fairly\"), snowball_stemmer.stem(\"sportingly\"), snowball_stemmer.stem(\"goes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPMU84Hvncvr"
      },
      "source": [
        "# Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWW0rMm1npii"
      },
      "source": [
        "**Wordnet Lemmatizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGbuCcxXnzGr"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M19nDoaYoFuW"
      },
      "outputs": [],
      "source": [
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIhL_mO-obla",
        "outputId": "463c9e83-43c2-4f54-a1a4-293ebab20a8f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 340,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7Vz8McJSoJMg",
        "outputId": "2926e1f7-0339-448b-fe88-9b2812c63ed0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'going'"
            ]
          },
          "execution_count": 341,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wordnet_lemmatizer.lemmatize(\"going\", \"n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BBDzmF6Colhz",
        "outputId": "51c4991f-2ccc-4a16-863f-4acc97a27d2f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'go'"
            ]
          },
          "execution_count": 342,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wordnet_lemmatizer.lemmatize(\"goes\", \"v\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "v9NauoR3pYFS",
        "outputId": "a392a626-55c7-41d7-bb08-78566f090bcc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'worker'"
            ]
          },
          "execution_count": 343,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wordnet_lemmatizer.lemmatize(\"worker\", \"v\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgC4O2N8phiK"
      },
      "source": [
        " Valid options are \"n\" for nouns,\n",
        "    \"v\" for verbs, \"a\" for adjectives, \"r\" for adverbs and \"s\"\n",
        "for satellite adjectives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7WQuuE_qdyC",
        "outputId": "e946c33f-cd38-4678-d525-c04539563ba5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "program --> program\n",
            "programs --> program\n",
            "programer --> programer\n",
            "programing --> programing\n",
            "programers --> programers\n"
          ]
        }
      ],
      "source": [
        "for word in words:\n",
        "  print(word+\" --> \"+wordnet_lemmatizer.lemmatize(word, \"n\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AInz1pCwp-i-",
        "outputId": "80eff340-54ce-46e9-95cd-15356c1a8ec9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "program --> program\n",
            "programs --> program\n",
            "programer --> programer\n",
            "programing --> program\n",
            "programers --> programers\n"
          ]
        }
      ],
      "source": [
        "for word in words:\n",
        "  print(word+\" --> \"+wordnet_lemmatizer.lemmatize(word, \"v\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_q_YsCNqxF8",
        "outputId": "001c3b0a-5e3f-436d-b090-4de67b6f245e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('fairly', 'sportingly', 'go')"
            ]
          },
          "execution_count": 346,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wordnet_lemmatizer.lemmatize(\"fairly\", \"r\"), wordnet_lemmatizer.lemmatize(\"sportingly\", \"r\"), wordnet_lemmatizer.lemmatize(\"goes\", \"v\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7jdbyItrZ3F"
      },
      "source": [
        "# Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDAPdL_Vr1Ul"
      },
      "outputs": [],
      "source": [
        "paragraph = '''Because I am ready today, as I did in the past, to bring any personal sacrifice, I do not demand of any German man to do anything I was not prepared to do myself for four years. There should be no deprivation in Germany that I will not share. My entire life belongs from this moment on to my people.'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUnDWZ44slX7",
        "outputId": "1190b114-684d-42ea-ebf6-52eec04eb383"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 348,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0KpRYAcsbzq"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DkgZhuZsvyW",
        "outputId": "21709e95-42a9-4641-9eee-be53031584a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " \"he's\",\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " 'if',\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " \"i've\",\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " \"should've\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " \"we've\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " 'your',\n",
              " \"you're\",\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " \"you've\"]"
            ]
          },
          "execution_count": 350,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "559PFgIOs6Pn",
        "outputId": "ec2faac7-0693-455c-f19e-2034ef47ae1e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['অতএব',\n",
              " 'অথচ',\n",
              " 'অথবা',\n",
              " 'অনুযায়ী',\n",
              " 'অনেক',\n",
              " 'অনেকে',\n",
              " 'অনেকেই',\n",
              " 'অন্তত',\n",
              " 'অন্য',\n",
              " 'অবধি',\n",
              " 'অবশ্য',\n",
              " 'অর্থাত',\n",
              " 'আই',\n",
              " 'আগামী',\n",
              " 'আগে',\n",
              " 'আগেই',\n",
              " 'আছে',\n",
              " 'আজ',\n",
              " 'আদ্যভাগে',\n",
              " 'আপনার',\n",
              " 'আপনি',\n",
              " 'আবার',\n",
              " 'আমরা',\n",
              " 'আমাকে',\n",
              " 'আমাদের',\n",
              " 'আমার',\n",
              " 'আমি',\n",
              " 'আর',\n",
              " 'আরও',\n",
              " 'ই',\n",
              " 'ইত্যাদি',\n",
              " 'ইহা',\n",
              " 'উচিত',\n",
              " 'উত্তর',\n",
              " 'উনি',\n",
              " 'উপর',\n",
              " 'উপরে',\n",
              " 'এ',\n",
              " 'এঁদের',\n",
              " 'এঁরা',\n",
              " 'এই',\n",
              " 'একই',\n",
              " 'একটি',\n",
              " 'একবার',\n",
              " 'একে',\n",
              " 'এক্',\n",
              " 'এখন',\n",
              " 'এখনও',\n",
              " 'এখানে',\n",
              " 'এখানেই',\n",
              " 'এটা',\n",
              " 'এটাই',\n",
              " 'এটি',\n",
              " 'এত',\n",
              " 'এতটাই',\n",
              " 'এতে',\n",
              " 'এদের',\n",
              " 'এব',\n",
              " 'এবং',\n",
              " 'এবার',\n",
              " 'এমন',\n",
              " 'এমনকী',\n",
              " 'এমনি',\n",
              " 'এর',\n",
              " 'এরা',\n",
              " 'এল',\n",
              " 'এস',\n",
              " 'এসে',\n",
              " 'ঐ',\n",
              " 'ও',\n",
              " 'ওঁদের',\n",
              " 'ওঁর',\n",
              " 'ওঁরা',\n",
              " 'ওই',\n",
              " 'ওকে',\n",
              " 'ওখানে',\n",
              " 'ওদের',\n",
              " 'ওর',\n",
              " 'ওরা',\n",
              " 'কখনও',\n",
              " 'কত',\n",
              " 'কবে',\n",
              " 'কমনে',\n",
              " 'কয়েক',\n",
              " 'কয়েকটি',\n",
              " 'করছে',\n",
              " 'করছেন',\n",
              " 'করতে',\n",
              " 'করবে',\n",
              " 'করবেন',\n",
              " 'করলে',\n",
              " 'করলেন',\n",
              " 'করা',\n",
              " 'করাই',\n",
              " 'করায়',\n",
              " 'করার',\n",
              " 'করি',\n",
              " 'করিতে',\n",
              " 'করিয়া',\n",
              " 'করিয়ে',\n",
              " 'করে',\n",
              " 'করেই',\n",
              " 'করেছিলেন',\n",
              " 'করেছে',\n",
              " 'করেছেন',\n",
              " 'করেন',\n",
              " 'কাউকে',\n",
              " 'কাছ',\n",
              " 'কাছে',\n",
              " 'কাজ',\n",
              " 'কাজে',\n",
              " 'কারও',\n",
              " 'কারণ',\n",
              " 'কি',\n",
              " 'কিংবা',\n",
              " 'কিছু',\n",
              " 'কিছুই',\n",
              " 'কিন্তু',\n",
              " 'কী',\n",
              " 'কে',\n",
              " 'কেউ',\n",
              " 'কেউই',\n",
              " 'কেখা',\n",
              " 'কেন',\n",
              " 'কোটি',\n",
              " 'কোন',\n",
              " 'কোনও',\n",
              " 'কোনো',\n",
              " 'ক্ষেত্রে',\n",
              " 'কয়েক',\n",
              " 'খুব',\n",
              " 'গিয়ে',\n",
              " 'গিয়েছে',\n",
              " 'গিয়ে',\n",
              " 'গুলি',\n",
              " 'গেছে',\n",
              " 'গেল',\n",
              " 'গেলে',\n",
              " 'গোটা',\n",
              " 'চলে',\n",
              " 'চান',\n",
              " 'চায়',\n",
              " 'চার',\n",
              " 'চালু',\n",
              " 'চেয়ে',\n",
              " 'চেষ্টা',\n",
              " 'ছাড়া',\n",
              " 'ছাড়াও',\n",
              " 'ছিল',\n",
              " 'ছিলেন',\n",
              " 'জন',\n",
              " 'জনকে',\n",
              " 'জনের',\n",
              " 'জন্য',\n",
              " 'জন্যওজে',\n",
              " 'জানতে',\n",
              " 'জানা',\n",
              " 'জানানো',\n",
              " 'জানায়',\n",
              " 'জানিয়ে',\n",
              " 'জানিয়েছে',\n",
              " 'জে',\n",
              " 'জ্নজন',\n",
              " 'টি',\n",
              " 'ঠিক',\n",
              " 'তখন',\n",
              " 'তত',\n",
              " 'তথা',\n",
              " 'তবু',\n",
              " 'তবে',\n",
              " 'তা',\n",
              " 'তাঁকে',\n",
              " 'তাঁদের',\n",
              " 'তাঁর',\n",
              " 'তাঁরা',\n",
              " 'তাঁাহারা',\n",
              " 'তাই',\n",
              " 'তাও',\n",
              " 'তাকে',\n",
              " 'তাতে',\n",
              " 'তাদের',\n",
              " 'তার',\n",
              " 'তারপর',\n",
              " 'তারা',\n",
              " 'তারৈ',\n",
              " 'তাহলে',\n",
              " 'তাহা',\n",
              " 'তাহাতে',\n",
              " 'তাহার',\n",
              " 'তিনঐ',\n",
              " 'তিনি',\n",
              " 'তিনিও',\n",
              " 'তুমি',\n",
              " 'তুলে',\n",
              " 'তেমন',\n",
              " 'তো',\n",
              " 'তোমার',\n",
              " 'থাকবে',\n",
              " 'থাকবেন',\n",
              " 'থাকা',\n",
              " 'থাকায়',\n",
              " 'থাকে',\n",
              " 'থাকেন',\n",
              " 'থেকে',\n",
              " 'থেকেই',\n",
              " 'থেকেও',\n",
              " 'দিকে',\n",
              " 'দিতে',\n",
              " 'দিন',\n",
              " 'দিয়ে',\n",
              " 'দিয়েছে',\n",
              " 'দিয়েছেন',\n",
              " 'দিলেন',\n",
              " 'দু',\n",
              " 'দুই',\n",
              " 'দুটি',\n",
              " 'দুটো',\n",
              " 'দেওয়া',\n",
              " 'দেওয়ার',\n",
              " 'দেওয়া',\n",
              " 'দেখতে',\n",
              " 'দেখা',\n",
              " 'দেখে',\n",
              " 'দেন',\n",
              " 'দেয়',\n",
              " 'দ্বারা',\n",
              " 'ধরা',\n",
              " 'ধরে',\n",
              " 'ধামার',\n",
              " 'নতুন',\n",
              " 'নয়',\n",
              " 'না',\n",
              " 'নাই',\n",
              " 'নাকি',\n",
              " 'নাগাদ',\n",
              " 'নানা',\n",
              " 'নিজে',\n",
              " 'নিজেই',\n",
              " 'নিজেদের',\n",
              " 'নিজের',\n",
              " 'নিতে',\n",
              " 'নিয়ে',\n",
              " 'নিয়ে',\n",
              " 'নেই',\n",
              " 'নেওয়া',\n",
              " 'নেওয়ার',\n",
              " 'নেওয়া',\n",
              " 'নয়',\n",
              " 'পক্ষে',\n",
              " 'পর',\n",
              " 'পরে',\n",
              " 'পরেই',\n",
              " 'পরেও',\n",
              " 'পর্যন্ত',\n",
              " 'পাওয়া',\n",
              " 'পাচ',\n",
              " 'পারি',\n",
              " 'পারে',\n",
              " 'পারেন',\n",
              " 'পি',\n",
              " 'পেয়ে',\n",
              " 'পেয়্র্',\n",
              " 'প্রতি',\n",
              " 'প্রথম',\n",
              " 'প্রভৃতি',\n",
              " 'প্রযন্ত',\n",
              " 'প্রাথমিক',\n",
              " 'প্রায়',\n",
              " 'প্রায়',\n",
              " 'ফলে',\n",
              " 'ফিরে',\n",
              " 'ফের',\n",
              " 'বক্তব্য',\n",
              " 'বদলে',\n",
              " 'বন',\n",
              " 'বরং',\n",
              " 'বলতে',\n",
              " 'বলল',\n",
              " 'বললেন',\n",
              " 'বলা',\n",
              " 'বলে',\n",
              " 'বলেছেন',\n",
              " 'বলেন',\n",
              " 'বসে',\n",
              " 'বহু',\n",
              " 'বা',\n",
              " 'বাদে',\n",
              " 'বার',\n",
              " 'বি',\n",
              " 'বিনা',\n",
              " 'বিভিন্ন',\n",
              " 'বিশেষ',\n",
              " 'বিষয়টি',\n",
              " 'বেশ',\n",
              " 'বেশি',\n",
              " 'ব্যবহার',\n",
              " 'ব্যাপারে',\n",
              " 'ভাবে',\n",
              " 'ভাবেই',\n",
              " 'মতো',\n",
              " 'মতোই',\n",
              " 'মধ্যভাগে',\n",
              " 'মধ্যে',\n",
              " 'মধ্যেই',\n",
              " 'মধ্যেও',\n",
              " 'মনে',\n",
              " 'মাত্র',\n",
              " 'মাধ্যমে',\n",
              " 'মোট',\n",
              " 'মোটেই',\n",
              " 'যখন',\n",
              " 'যত',\n",
              " 'যতটা',\n",
              " 'যথেষ্ট',\n",
              " 'যদি',\n",
              " 'যদিও',\n",
              " 'যা',\n",
              " 'যাঁর',\n",
              " 'যাঁরা',\n",
              " 'যাওয়া',\n",
              " 'যাওয়ার',\n",
              " 'যাওয়া',\n",
              " 'যাকে',\n",
              " 'যাচ্ছে',\n",
              " 'যাতে',\n",
              " 'যাদের',\n",
              " 'যান',\n",
              " 'যাবে',\n",
              " 'যায়',\n",
              " 'যার',\n",
              " 'যারা',\n",
              " 'যিনি',\n",
              " 'যে',\n",
              " 'যেখানে',\n",
              " 'যেতে',\n",
              " 'যেন',\n",
              " 'যেমন',\n",
              " 'র',\n",
              " 'রকম',\n",
              " 'রয়েছে',\n",
              " 'রাখা',\n",
              " 'রেখে',\n",
              " 'লক্ষ',\n",
              " 'শুধু',\n",
              " 'শুরু',\n",
              " 'সঙ্গে',\n",
              " 'সঙ্গেও',\n",
              " 'সব',\n",
              " 'সবার',\n",
              " 'সমস্ত',\n",
              " 'সম্প্রতি',\n",
              " 'সহ',\n",
              " 'সহিত',\n",
              " 'সাধারণ',\n",
              " 'সামনে',\n",
              " 'সি',\n",
              " 'সুতরাং',\n",
              " 'সে',\n",
              " 'সেই',\n",
              " 'সেখান',\n",
              " 'সেখানে',\n",
              " 'সেটা',\n",
              " 'সেটাই',\n",
              " 'সেটাও',\n",
              " 'সেটি',\n",
              " 'স্পষ্ট',\n",
              " 'স্বয়ং',\n",
              " 'হইতে',\n",
              " 'হইবে',\n",
              " 'হইয়া',\n",
              " 'হওয়া',\n",
              " 'হওয়ায়',\n",
              " 'হওয়ার',\n",
              " 'হচ্ছে',\n",
              " 'হত',\n",
              " 'হতে',\n",
              " 'হতেই',\n",
              " 'হন',\n",
              " 'হবে',\n",
              " 'হবেন',\n",
              " 'হয়',\n",
              " 'হয়তো',\n",
              " 'হয়নি',\n",
              " 'হয়ে',\n",
              " 'হয়েই',\n",
              " 'হয়েছিল',\n",
              " 'হয়েছে',\n",
              " 'হয়েছেন',\n",
              " 'হল',\n",
              " 'হলে',\n",
              " 'হলেই',\n",
              " 'হলেও',\n",
              " 'হলো',\n",
              " 'হাজার',\n",
              " 'হিসাবে',\n",
              " 'হৈলে',\n",
              " 'হোক',\n",
              " 'হয়']"
            ]
          },
          "execution_count": 351,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stopwords.words('bengali')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVjQCzGgtJ4I"
      },
      "outputs": [],
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGzrnYRBtkQN",
        "outputId": "ef895afc-0493-4179-b0b3-45c0cae4d2a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Because I am ready today, as I did in the past, to bring any personal sacrifice, I do not demand of any German man to do anything I was not prepared to do myself for four years.',\n",
              " 'There should be no deprivation in Germany that I will not share.',\n",
              " 'My entire life belongs from this moment on to my people.']"
            ]
          },
          "execution_count": 353,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQvDnOg9tu3c",
        "outputId": "f3909bbb-0224-45e5-9997-5507ebfa20f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 354,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGjhLySWt4dO"
      },
      "source": [
        "**Stopwords -> Filter -> Stemming**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f17ebd9",
        "outputId": "96ec8096-1a48-4441-80bc-421ae63228a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "readi today , past , bring person sacrific , demand german man anyth prepar four year .\n",
            "depriv germani share .\n",
            "entir life belong moment peopl .\n"
          ]
        }
      ],
      "source": [
        "# List to store the filtered and stemmed sentences\n",
        "processed_sentences = []\n",
        "\n",
        "# Iterate through each sentence\n",
        "for sentence in sentences:\n",
        "  # Tokenize the sentence into words\n",
        "  words = nltk.word_tokenize(sentence)\n",
        "  # Filter out stopwords and perform stemming\n",
        "  filtered_and_stemmed_words = [\n",
        "      porter_stemmer.stem(word) for word in words if word.lower() not in set(stopwords.words('english'))\n",
        "  ]\n",
        "\n",
        "  # Join the processed words back into a sentence and add to the list\n",
        "  processed_sentences.append(' '.join(filtered_and_stemmed_words))\n",
        "\n",
        "# Print the processed sentences\n",
        "for sentence in processed_sentences:\n",
        "  print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awMjuhnOx-ax",
        "outputId": "1a543ab7-c4d9-4506-cb61-7a3054bdf50c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['entir', 'life', 'belong', 'moment', 'peopl', '.']"
            ]
          },
          "execution_count": 356,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "filtered_and_stemmed_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8LkBYmvyVGE",
        "outputId": "c31178d6-897f-4b71-d455-e1cfead5666a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "readi today , past , bring person sacrific , demand german man anyth prepar four year .\n",
            "depriv germani share .\n",
            "entir life belong moment peopl .\n"
          ]
        }
      ],
      "source": [
        "# List to store the filtered and stemmed sentences\n",
        "processed_sentences = []\n",
        "\n",
        "# Iterate through each sentence\n",
        "for sentence in sentences:\n",
        "  # Tokenize the sentence into words\n",
        "  words = nltk.word_tokenize(sentence)\n",
        "  # Filter out stopwords and perform stemming\n",
        "  filtered_and_stemmed_words = [\n",
        "      snowball_stemmer.stem(word) for word in words if word.lower() not in set(stopwords.words('english'))\n",
        "  ]\n",
        "\n",
        "  # Join the processed words back into a sentence and add to the list\n",
        "  processed_sentences.append(' '.join(filtered_and_stemmed_words))\n",
        "\n",
        "# Print the processed sentences\n",
        "for sentence in processed_sentences:\n",
        "  print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWrNWSUqy9YH",
        "outputId": "0004d1bc-0f6e-444c-fe3a-293c81301239"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ready today , past , bring personal sacrifice , demand German man anything prepared four year .\n",
            "deprivation Germany share .\n",
            "entire life belongs moment people .\n"
          ]
        }
      ],
      "source": [
        "# List to store the filtered and lemmatized sentences\n",
        "processed_sentences = []\n",
        "\n",
        "# Create an instance of WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Iterate through each sentence\n",
        "for sentence in sentences:\n",
        "  # Tokenize the sentence into words\n",
        "  words = nltk.word_tokenize(sentence)\n",
        "  # Filter out stopwords and perform lemmatization\n",
        "  filtered_and_lemmatized_words = [\n",
        "      wordnet_lemmatizer.lemmatize(word, pos='n') for word in words if word.lower() not in set(stopwords.words('english'))\n",
        "  ]\n",
        "\n",
        "  # Join the processed words back into a sentence and add to the list\n",
        "  processed_sentences.append(' '.join(filtered_and_lemmatized_words))\n",
        "\n",
        "# Print the processed sentences\n",
        "for sentence in processed_sentences:\n",
        "  print(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part-of-Speech (POS) Tags in NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| **Tag**  | **Meaning**                         | **Example**              |\n",
        "|----------|--------------------------------------|--------------------------|\n",
        "| `CC`     | Coordinating conjunction             | and, but, or             |\n",
        "| `CD`     | Cardinal number                      | one, two, 100            |\n",
        "| `DT`     | Determiner                           | the, a, an, this         |\n",
        "| `EX`     | Existential there                    | there (in \"there is\")    |\n",
        "| `FW`     | Foreign word                         | d’abord, Zeitgeist       |\n",
        "| `IN`     | Preposition / Subordinating conj.    | in, of, like, although   |\n",
        "| `JJ`     | Adjective                            | happy, quick             |\n",
        "| `JJR`    | Adjective, comparative               | better, faster           |\n",
        "| `JJS`    | Adjective, superlative               | best, fastest            |\n",
        "| `LS`     | List item marker                     | 1), a.                   |\n",
        "| `MD`     | Modal                                | can, should, will        |\n",
        "| `NN`     | Noun, singular or mass               | dog, house               |\n",
        "| `NNS`    | Noun, plural                         | dogs, houses             |\n",
        "| `NNP`    | Proper noun, singular                | John, London             |\n",
        "| `NNPS`   | Proper noun, plural                  | Americans, Beatles       |\n",
        "| `PDT`    | Predeterminer                        | all, both                |\n",
        "| `POS`    | Possessive ending                    | ’s                       |\n",
        "| `PRP`    | Personal pronoun                     | I, he, she               |\n",
        "| `PRP$`   | Possessive pronoun                   | my, his, their           |\n",
        "| `RB`     | Adverb                               | quickly, never           |\n",
        "| `RBR`    | Adverb, comparative                  | faster, better           |\n",
        "| `RBS`    | Adverb, superlative                  | best, fastest            |\n",
        "| `RP`     | Particle                             | up, off, over            |\n",
        "| `SYM`    | Symbol                               | $, %, =                 |\n",
        "| `TO`     | to                                   | to (go to school)        |\n",
        "| `UH`     | Interjection                         | oh, wow, hey             |\n",
        "| `VB`     | Verb, base form                      | eat, run                 |\n",
        "| `VBD`    | Verb, past tense                     | ate, ran                 |\n",
        "| `VBG`    | Verb, gerund/present participle      | eating, running          |\n",
        "| `VBN`    | Verb, past participle                | eaten, run               |\n",
        "| `VBP`    | Verb, non-3rd person present         | eat, run                 |\n",
        "| `VBZ`    | Verb, 3rd person present             | eats, runs               |\n",
        "| `WDT`    | Wh-determiner                        | which, that              |\n",
        "| `WP`     | Wh-pronoun                           | who, what                |\n",
        "| `WP$`    | Possessive wh-pronoun                | whose                    |\n",
        "| `WRB`    | Wh-adverb                            | when, where, why         |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "paragraph = '''Because I am ready today, as I did in the past, to bring any personal sacrifice, I do not demand of any German man to do anything I was not prepared to do myself for four years. There should be no deprivation in Germany that I will not share. My entire life belongs from this moment on to my people.'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting click (from nltk)\n",
            "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting joblib (from nltk)\n",
            "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk)\n",
            "  Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
            "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
            "     ------------------ ------------------- 20.5/41.5 kB 330.3 kB/s eta 0:00:01\n",
            "     ---------------------------- --------- 30.7/41.5 kB 262.6 kB/s eta 0:00:01\n",
            "     -------------------------------------- 41.5/41.5 kB 284.6 kB/s eta 0:00:00\n",
            "Collecting tqdm (from nltk)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->nltk) (0.4.6)\n",
            "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "Downloading regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
            "   ---------------------------------------- 0.0/274.1 kB ? eta -:--:--\n",
            "   ----- --------------------------------- 41.0/274.1 kB 991.0 kB/s eta 0:00:01\n",
            "   ------------------------- -------------- 174.1/274.1 kB 1.7 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 266.2/274.1 kB 1.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 274.1/274.1 kB 1.7 MB/s eta 0:00:00\n",
            "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
            "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
            "Successfully installed click-8.2.1 joblib-1.5.1 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
            "[notice] To update, run: C:\\Users\\HP\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk\n",
        "\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "sentences = nltk.sent_tokenize(paragraph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Because I am ready today, as I did in the past, to bring any personal sacrifice, I do not demand of any German man to do anything I was not prepared to do myself for four years.',\n",
              " 'There should be no deprivation in Germany that I will not share.',\n",
              " 'My entire life belongs from this moment on to my people.']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply Stopwords, filter and stem the sentences\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [porter_stemmer.stem(word) for word in words if word.lower() not in set(stopwords.words('english'))]\n",
        "    sentences[i] = ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('readi', 'NN'), ('today', 'NN'), (',', ','), ('past', 'NN'), (',', ','), ('bring', 'VBG'), ('person', 'NN'), ('sacrif', 'NN'), (',', ','), ('demand', 'NN'), ('german', 'JJ'), ('man', 'NN'), ('anyth', 'VBZ'), ('prepar', 'RB'), ('four', 'CD'), ('year', 'NN'), ('.', '.')]\n",
            "[('depriv', 'NN'), ('germani', 'NN'), ('share', 'NN'), ('.', '.')]\n",
            "[('entir', 'JJ'), ('life', 'NN'), ('belong', 'JJ'), ('moment', 'NN'), ('peopl', 'NN'), ('.', '.')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [porter_stemmer.stem(word) for word in words if word.lower() not in set(stopwords.words('english'))]\n",
        "    pos_tagged_words = nltk.pos_tag(words, lang='eng')\n",
        "    print(pos_tagged_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence = \"Bangladesh is a riverine country in South Asia. It is bordered by India to the west, north, and east, Myanmar to the southeast, and the Bay of Bengal to the south.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[('Bangladesh', 'NNP'),\n",
              "  ('is', 'VBZ'),\n",
              "  ('a', 'DT'),\n",
              "  ('riverine', 'JJ'),\n",
              "  ('country', 'NN'),\n",
              "  ('in', 'IN'),\n",
              "  ('South', 'NNP'),\n",
              "  ('Asia', 'NNP'),\n",
              "  ('.', '.'),\n",
              "  ('It', 'PRP'),\n",
              "  ('is', 'VBZ'),\n",
              "  ('bordered', 'VBN'),\n",
              "  ('by', 'IN'),\n",
              "  ('India', 'NNP'),\n",
              "  ('to', 'TO'),\n",
              "  ('the', 'DT'),\n",
              "  ('west', 'NN'),\n",
              "  (',', ','),\n",
              "  ('north', 'JJ'),\n",
              "  (',', ','),\n",
              "  ('and', 'CC'),\n",
              "  ('east', 'RB'),\n",
              "  (',', ','),\n",
              "  ('Myanmar', 'NNP'),\n",
              "  ('to', 'TO'),\n",
              "  ('the', 'DT'),\n",
              "  ('southeast', 'NN'),\n",
              "  (',', ','),\n",
              "  ('and', 'CC'),\n",
              "  ('the', 'DT'),\n",
              "  ('Bay', 'NNP'),\n",
              "  ('of', 'IN'),\n",
              "  ('Bengal', 'NNP'),\n",
              "  ('to', 'TO'),\n",
              "  ('the', 'DT'),\n",
              "  ('south', 'NN'),\n",
              "  ('.', '.')]]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "# Tokenize the sentence first, then use pos_tag_sents which expects a list of sentences (each as a list of tokens)\n",
        "nltk.word_tokenize(sentence)\n",
        "nltk.pos_tag_sents([tokens], lang='eng')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Bangladesh', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('riverine', 'JJ'), ('country', 'NN'), ('in', 'IN'), ('South', 'NNP'), ('Asia.', 'NNP'), ('It', 'PRP'), ('is', 'VBZ'), ('bordered', 'VBN'), ('by', 'IN'), ('India', 'NNP'), ('to', 'TO'), ('the', 'DT'), ('west,', 'NN'), ('north,', 'NN'), ('and', 'CC'), ('east,', 'JJ'), ('Myanmar', 'NNP'), ('to', 'TO'), ('the', 'DT'), ('southeast,', 'NN'), ('and', 'CC'), ('the', 'DT'), ('Bay', 'NNP'), ('of', 'IN'), ('Bengal', 'NNP'), ('to', 'TO'), ('the', 'DT'), ('south.', 'NN')]\n"
          ]
        }
      ],
      "source": [
        "print(nltk.pos_tag(sentence.split()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "Barack Obama was born in Hawaii. He was elected president of the United States in 2008. \n",
        "Google was founded in 1998 and is headquartered in Mountain View, California.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PERSON: ['Barack Obama', 'United States', 'Mountain View']\n",
            "ORG: ['Google']\n",
            "GPE (Location): ['Hawaii', 'California', 'United States', 'Mountain View']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Define rule-based patterns\n",
        "person_pattern = r\"\\b[A-Z][a-z]+ [A-Z][a-z]+\\b\"\n",
        "organization_keywords = [\"Google\", \"Microsoft\", \"Apple\"]\n",
        "location_keywords = [\"Hawaii\", \"California\", \"United States\", \"Mountain View\"]\n",
        "\n",
        "# Match PERSON\n",
        "persons = re.findall(person_pattern, text)\n",
        "\n",
        "# Match ORG, GPE based on keyword lists\n",
        "orgs = [word for word in organization_keywords if word in text]\n",
        "locations = [word for word in location_keywords if word in text]\n",
        "\n",
        "# Print results\n",
        "print(\"PERSON:\", persons)\n",
        "print(\"ORG:\", orgs)\n",
        "print(\"GPE (Location):\", locations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Practice*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "my_data = '''\n",
        "I'm Sk. Roushan Khalid, passionate about Artificial Intelligence, Machine Learning, and Web Development, constantly pushing my limits to build data-driven solutions. I always love turning my ideas into reality.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I',\n",
              " \"'m\",\n",
              " 'Sk',\n",
              " '.',\n",
              " 'Roushan',\n",
              " 'Khalid',\n",
              " ',',\n",
              " 'passionate',\n",
              " 'about',\n",
              " 'Artificial',\n",
              " 'Intelligence',\n",
              " ',',\n",
              " 'Machine',\n",
              " 'Learning',\n",
              " ',',\n",
              " 'and',\n",
              " 'Web',\n",
              " 'Development',\n",
              " ',',\n",
              " 'constantly',\n",
              " 'pushing',\n",
              " 'my',\n",
              " 'limits',\n",
              " 'to',\n",
              " 'build',\n",
              " 'data-driven',\n",
              " 'solutions',\n",
              " '.',\n",
              " 'I',\n",
              " 'always',\n",
              " 'love',\n",
              " 'turning',\n",
              " 'my',\n",
              " 'ideas',\n",
              " 'into',\n",
              " 'reality',\n",
              " '.']"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.word_tokenize(my_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "tagged_from_my_data = nltk.pos_tag(nltk.word_tokenize(my_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " (\"'m\", 'VBP'),\n",
              " ('Sk', 'JJ'),\n",
              " ('.', '.'),\n",
              " ('Roushan', 'NNP'),\n",
              " ('Khalid', 'NNP'),\n",
              " (',', ','),\n",
              " ('passionate', 'NN'),\n",
              " ('about', 'IN'),\n",
              " ('Artificial', 'NNP'),\n",
              " ('Intelligence', 'NNP'),\n",
              " (',', ','),\n",
              " ('Machine', 'NNP'),\n",
              " ('Learning', 'NNP'),\n",
              " (',', ','),\n",
              " ('and', 'CC'),\n",
              " ('Web', 'NNP'),\n",
              " ('Development', 'NNP'),\n",
              " (',', ','),\n",
              " ('constantly', 'RB'),\n",
              " ('pushing', 'VBG'),\n",
              " ('my', 'PRP$'),\n",
              " ('limits', 'NNS'),\n",
              " ('to', 'TO'),\n",
              " ('build', 'VB'),\n",
              " ('data-driven', 'JJ'),\n",
              " ('solutions', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('I', 'PRP'),\n",
              " ('always', 'RB'),\n",
              " ('love', 'VBP'),\n",
              " ('turning', 'VBG'),\n",
              " ('my', 'PRP$'),\n",
              " ('ideas', 'NNS'),\n",
              " ('into', 'IN'),\n",
              " ('reality', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tagged_from_my_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to\n",
            "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PERSON: ['Sk. Roushan Khalid', 'Artificial Intelligence', 'Machine Learning', 'Web Development']\n",
            "Interested in: ['Artificial Intelligence', 'Machine Learning', 'Web Development']\n",
            "GPE (Location): []\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "my_data = '''\n",
        "I'm Sk. Roushan Khalid, passionate about Artificial Intelligence, Machine Learning, and Web Development, constantly pushing my limits to build data-driven solutions. I always love turning my ideas into reality.\n",
        "'''\n",
        "\n",
        "# Improved pattern to match names with optional titles/initials\n",
        "person_pattern = r\"\\b(?:[A-Z][a-z]*\\.?\\s)?[A-Z][a-z]+ [A-Z][a-z]+\\b\"\n",
        "interested_in_ = [\n",
        "    \"Cyber Security\", \"DevOps\", \"Data Science\", \"Artificial Intelligence\",\n",
        "    \"Machine Learning\", \"Web Development\", \"Cloud Computing\", \"Blockchain\",\n",
        "    \"Internet of Things (IoT)\", \"Augmented Reality (AR)\", \"Virtual Reality (VR)\"\n",
        "]\n",
        "location_keywords = [\n",
        "    \"Dhaka\", \"Bangladesh\", \"India\", \"USA\", \"California\", \"New York\", \"London\", \"Tokyo\"\n",
        "]\n",
        "\n",
        "# Match PERSON in my_data\n",
        "persons = re.findall(person_pattern, my_data)\n",
        "\n",
        "# Match \"Interested in\" and GPE (Location) based on keyword lists\n",
        "interests = [word for word in interested_in_ if word in my_data]\n",
        "locations = [word for word in location_keywords if word in my_data]\n",
        "\n",
        "# Print results\n",
        "print(\"PERSON:\", persons)\n",
        "print(\"Interested in:\", interests)\n",
        "print(\"GPE (Location):\", locations)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
